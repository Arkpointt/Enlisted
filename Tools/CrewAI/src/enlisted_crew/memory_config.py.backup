"""
Enlisted CrewAI Memory Configuration

Provides optimized memory settings that prevent embedding token limit errors
while keeping all memory types enabled.

The Problem:
- Short-Term and Entity memory use embeddings (ChromaDB + RAG)
- OpenAI embedding models have 8,192 token limit per request
- Large agent outputs (9,000+ tokens) crash the embedding API

The Solution:
- Truncate content before embedding to stay under the limit
- Use a safety margin (7,500 tokens) to account for overhead
- Long-Term Memory (SQLite) has no token limits, works as-is

Usage:
    from enlisted_crew.memory_config import get_memory_config
    
    crew = Crew(
        agents=[...],
        tasks=[...],
        **get_memory_config(),  # Unpacks all memory settings
    )
"""

import os
from pathlib import Path
from typing import Any, Dict, Optional

# Token limit for OpenAI embedding models
EMBEDDING_TOKEN_LIMIT = 8192
SAFETY_MARGIN = 700  # Buffer for metadata, formatting
MAX_CONTENT_TOKENS = EMBEDDING_TOKEN_LIMIT - SAFETY_MARGIN  # 7,492


def _get_storage_path() -> Path:
    """Get the CrewAI storage path for Enlisted."""
    # Use CREWAI_STORAGE_DIR if set, otherwise default location
    storage_dir = os.environ.get("CREWAI_STORAGE_DIR")
    if storage_dir:
        return Path(storage_dir)
    
    # Default: AppData/Local/CrewAI/enlisted_crew on Windows
    local_app_data = os.environ.get("LOCALAPPDATA", "")
    if local_app_data:
        return Path(local_app_data) / "CrewAI" / "enlisted_crew"
    
    # Fallback for non-Windows
    return Path.home() / ".crewai" / "enlisted_crew"


def get_embedder_config() -> Dict[str, Any]:
    """Get the embedder configuration for memory.
    
    Uses text-embedding-3-large for best semantic quality.
    Both small and large have the same 8,192 token limit.
    The truncating storage handles oversized content.
    """
    return {
        "provider": "openai",
        "config": {
            "model": "text-embedding-3-large",  # 3,072 dimensions, best quality
        }
    }


def get_memory_config(use_truncating_storage: bool = True) -> Dict[str, Any]:
    """Get optimized memory configuration for Enlisted crews.
    
    Returns a dict that can be unpacked into Crew():
        crew = Crew(**get_memory_config(), agents=[...], tasks=[...])
    
    Configuration:
    - memory=True: Enable all memory types
    - embedder: text-embedding-3-small (faster, sufficient quality)
    - Long-Term Memory: SQLite (no token limits)
    - Short-Term/Entity: Custom truncating storage to prevent token limit errors
    
    Args:
        use_truncating_storage: If True, use custom storage that truncates
            content before embedding. This prevents the 8,192 token limit
            error that occurs with large agent outputs. Default True.
    
    Note: When use_truncating_storage=True, content longer than ~7,500 tokens
    will be truncated before saving to Short-Term and Entity memory.
    Long-Term Memory (SQLite) is unaffected as it doesn't use embeddings.
    """
    config = {
        "memory": True,
        "embedder": get_embedder_config(),
    }
    
    if use_truncating_storage:
        # Use custom storage with truncation
        short_term = get_truncating_short_term_memory()
        entity = get_truncating_entity_memory()
        
        if short_term:
            config["short_term_memory"] = short_term
        if entity:
            config["entity_memory"] = entity
    
    return config


def truncate_for_embedding(content: str, max_tokens: int = MAX_CONTENT_TOKENS) -> str:
    """Truncate content to fit within embedding token limits.
    
    Uses tiktoken for accurate token counting with cl100k_base encoding
    (used by text-embedding-3-small and text-embedding-3-large).
    
    Args:
        content: The text to potentially truncate
        max_tokens: Maximum tokens allowed (default: 7,492)
    
    Returns:
        Content truncated to fit within token limit, with indicator if truncated
    """
    try:
        import tiktoken
        encoding = tiktoken.get_encoding("cl100k_base")
    except ImportError:
        # Fallback: rough estimate of 4 chars per token
        if len(content) > max_tokens * 4:
            truncated = content[:max_tokens * 4]
            return truncated + "\n\n[... truncated for embedding ...]"
        return content
    
    tokens = encoding.encode(content)
    
    if len(tokens) <= max_tokens:
        return content
    
    # Truncate and decode
    truncated_tokens = tokens[:max_tokens - 10]  # Leave room for truncation notice
    truncated_content = encoding.decode(truncated_tokens)
    
    return truncated_content + "\n\n[... truncated for embedding ...]"


# =============================================================================
# CUSTOM STORAGE WITH TRUNCATION
# =============================================================================

def get_truncating_short_term_memory():
    """Get Short-Term Memory with automatic truncation.
    
    Wraps the default RAGStorage to truncate content before embedding,
    preventing the 8,192 token limit error.
    """
    try:
        from crewai.memory.short_term.short_term_memory import ShortTermMemory
        from crewai.memory.storage.rag_storage import RAGStorage
        
        class TruncatingRAGStorage(RAGStorage):
            """RAGStorage that truncates content before embedding."""
            
            def save(self, value, metadata=None, agent=None):
                """Save with truncation to prevent embedding token limit errors."""
                if isinstance(value, str):
                    original_len = len(value)
                    value = truncate_for_embedding(value)
                    if len(value) < original_len:
                        print(f"[MEMORY] Truncated content from {original_len} to {len(value)} chars")
                return super().save(value, metadata, agent)
        
        storage = TruncatingRAGStorage(
            type="short_term",
            embedder_config=get_embedder_config(),
        )
        return ShortTermMemory(storage=storage)
    except ImportError as e:
        print(f"[MEMORY] Warning: Could not create truncating storage: {e}")
        return None


def get_truncating_entity_memory():
    """Get Entity Memory with automatic truncation.
    
    Wraps the default RAGStorage to truncate content before embedding,
    preventing the 8,192 token limit error.
    """
    try:
        from crewai.memory.entity.entity_memory import EntityMemory
        from crewai.memory.storage.rag_storage import RAGStorage
        
        class TruncatingRAGStorage(RAGStorage):
            """RAGStorage that truncates content before embedding."""
            
            def save(self, value, metadata=None, agent=None):
                """Save with truncation to prevent embedding token limit errors."""
                if isinstance(value, str):
                    original_len = len(value)
                    value = truncate_for_embedding(value)
                    if len(value) < original_len:
                        print(f"[MEMORY] Truncated entity from {original_len} to {len(value)} chars")
                return super().save(value, metadata, agent)
        
        storage = TruncatingRAGStorage(
            type="entities",
            embedder_config=get_embedder_config(),
        )
        return EntityMemory(storage=storage)
    except ImportError as e:
        print(f"[MEMORY] Warning: Could not create truncating entity storage: {e}")
        return None


def estimate_tokens(content: str) -> int:
    """Estimate token count for content.
    
    Uses tiktoken for accurate counting, falls back to character estimate.
    """
    try:
        import tiktoken
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(content))
    except ImportError:
        # Rough estimate: ~4 characters per token
        return len(content) // 4


def clear_memory(memory_types: Optional[list] = None) -> Dict[str, bool]:
    """Clear CrewAI memory storage.
    
    Args:
        memory_types: List of types to clear. Options:
            - 'short': Short-term memory (ChromaDB)
            - 'long': Long-term memory (SQLite)
            - 'entity': Entity memory (ChromaDB)
            - 'all': Clear everything
            If None, clears all.
    
    Returns:
        Dict of {memory_type: success_bool}
    """
    import shutil
    
    storage_path = _get_storage_path()
    
    if memory_types is None or 'all' in memory_types:
        memory_types = ['short', 'long', 'entity']
    
    type_to_folder = {
        'short': 'short_term_memory',
        'long': 'long_term_memory',
        'entity': 'entities',
    }
    
    results = {}
    
    for mem_type in memory_types:
        folder_name = type_to_folder.get(mem_type)
        if not folder_name:
            continue
            
        folder_path = storage_path / folder_name
        try:
            if folder_path.exists():
                shutil.rmtree(folder_path)
                results[mem_type] = True
                print(f"[MEMORY] Cleared {mem_type} memory")
            else:
                results[mem_type] = True  # Nothing to clear
        except Exception as e:
            results[mem_type] = False
            print(f"[MEMORY] Failed to clear {mem_type}: {e}")
    
    # Also clear SQLite DB for long-term
    if 'long' in memory_types:
        db_path = storage_path / "long_term_memory_storage.db"
        try:
            if db_path.exists():
                db_path.unlink()
                print("[MEMORY] Cleared long-term memory database")
        except Exception as e:
            print(f"[MEMORY] Failed to clear LTM database: {e}")
    
    return results


# Export for convenience
__all__ = [
    'get_memory_config',
    'get_embedder_config',
    'get_truncating_short_term_memory',
    'get_truncating_entity_memory',
    'truncate_for_embedding',
    'estimate_tokens',
    'clear_memory',
    'MAX_CONTENT_TOKENS',
    'EMBEDDING_TOKEN_LIMIT',
]
